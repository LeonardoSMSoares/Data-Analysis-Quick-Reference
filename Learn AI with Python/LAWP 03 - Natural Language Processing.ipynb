{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48ef33d-e339-4da1-a41b-ec8435e97eee",
   "metadata": {},
   "source": [
    "# Chapter 6 - Natural Language Processing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ecae07-0768-43fe-93e8-083fa598db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Library\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d9859f-1037-45dc-a405-990d7532dd32",
   "metadata": {},
   "source": [
    "### Tokenizing Sentences - `Word_tokenize()`\n",
    "This module is used for the basic tokenization of\r",
    "sentences into words. Let us understand it’s working with the help\r",
    "of the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc262ac6-10f8-4a42-a87f-05826bf7e246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'module',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'for',\n",
       " 'basic',\n",
       " 'tokenizing',\n",
       " 'of',\n",
       " 'sentences',\n",
       " 'into',\n",
       " 'words',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "word_tokenize(\"This module can be used for basic tokenizing of sentences into words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02da05-c946-4349-9a7a-3cf0cafe60fe",
   "metadata": {},
   "source": [
    "### Tokenizing Sentences - `TreebankWordTokenizer()`\n",
    "\n",
    "This method is invoked by *word_tokenize()* module with an assumption that the text has already been segmented into different sentences. The following are some characteristics of the treebank tokenizer method:\n",
    "\n",
    "* It splits standard contractions. For example, can’t -> ca n’t.\n",
    "\n",
    "* When followed by whitespaces, this method also splits off commas and single quotes.\n",
    "\n",
    "* It separates periods that appear at the end of the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78937412-9040-4395-ae7c-0d81ba1f634e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'vegan',\n",
       " 'pizza',\n",
       " 'cost',\n",
       " '$',\n",
       " '12.25',\n",
       " 'in',\n",
       " 'Pheonix',\n",
       " ',',\n",
       " 'Arizona.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them.',\n",
       " 'Thank',\n",
       " 'you',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "sentence = '''Good vegan pizza cost $12.25\\nin Pheonix, Arizona.  Please buy me\\ntwo of them.\\nThank you.'''\n",
    "\n",
    "TreebankWordTokenizer().tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a47800-dc33-4e0e-a29e-dab9d384914d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', \"'ll\", 'save', 'and', 'invest', 'for', 'his', 'retirement', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = \"He'll save and invest for his retirement.\"\n",
    "TreebankWordTokenizer().tokenize(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e63d5f07-e487-45f5-a745-895b391eaa37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'he', 'ca', \"n't\", 'go', 'to', 'market', ',']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2 = \"Hello, he can't go to market,\"\n",
    "TreebankWordTokenizer().tokenize(sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff7db0-d07e-45f1-bf53-d8da71f3495b",
   "metadata": {},
   "source": [
    "### Tokenizing Sentences - `WordPunctTokenizer()`\n",
    "As the name implies, this tokenizer splits\r",
    "punctuations into separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8960088a-0088-4bc6-9422-fb738789d892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', \"'\", 'll', 'save', 'and', 'invest', 'for', 'his', 'retirement', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "sentence = \"He'll save and invest for his retirement.\"\n",
    "WordPunctTokenizer().tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288c1e2-a753-4ae3-b8f9-1daacf94f2a5",
   "metadata": {},
   "source": [
    "### Tokenizing Sentences - `RegexpTokenizer()`\n",
    "As the name implies, this method uses a regular\r",
    "expression for tokenizing sentences into words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8af86b49-b0e3-4296-9bec-7e5eb1eeebcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"He'll\", 'save', 'and', 'invest', 'for', 'his', 'retirement']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "sentence = \"He'll save and invest for his retirement.\"\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e1b5a-f879-4ee1-98bb-5247ff9dcf9b",
   "metadata": {},
   "source": [
    "### Tokenizing Paragraphs - `sent_tokenize()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d26b5-8ecb-46ff-bc90-c5345a2debf3",
   "metadata": {},
   "source": [
    "We understood word tokenization, that is,\n",
    "tokenizing sentences into words with the help of the word_tokenizer()\n",
    "module. Now, we will tokenize paragraphs, that is, tokenizing paragraphs\n",
    "into sentences. For this, NLTK provides us the sent_tokenize() module.\n",
    "    \n",
    "But here the question arises if we have a word tokenizer then why do we\n",
    "need a sentence tokenizer? Both are important in one or another way. For\n",
    "example, if you would need to count average words in sentences, then\n",
    "you need both a sentence tokenizer as well as a word tokenizer. Let us\n",
    "understand its working and how sentence tokenizer is different from\n",
    "word tokenizer with the help of the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7247760e-04c0-4ca1-9b9e-e3a8cedeca74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It shows the difference between word tokenizer and sentence tokenizer.',\n",
       " \"It's a simple example.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"It shows the difference between word tokenizer and sentence tokenizer. It's a simple example.\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90254e9f-a6c0-4402-9f33-6095155c2e87",
   "metadata": {},
   "source": [
    "### Stemming - `PorterStemmer`\n",
    "NLTK provides us *PorterStemmer* class with the help\r",
    "of which we can easily implement a porter stemming algorithm, which is\r",
    "designed to remove as well as replace suffixes of English words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d57db8c3-5f53-44b3-8532-3d8df4b37d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemming_word = PorterStemmer()\n",
    "\n",
    "stemming_word.stem('writing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22c6ad0e-9fc9-4c17-b228-c67ee02d107d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming_word.stem('working')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c9824-f134-4a93-ba99-bd04a138c8d6",
   "metadata": {},
   "source": [
    "### Stemming-`LancasterStemmer`\n",
    "Another common stemming algorithm is the\r",
    "Lancaster Stemming algorithm, which is developed at Lancaster\r",
    "University. NLTK provides us the *LancasterStemmer* class with the help\r",
    "of which we can easily implement this algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f00537d0-edb1-439d-8bf0-394f028f4617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemming_Lanc = LancasterStemmer()\n",
    "\n",
    "stemming_Lanc.stem('reads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24f29cb2-44b5-482b-aa5a-0fe48b301e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sweet'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming_Lanc.stem('sweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d6366-c350-4b7b-8406-c8452959fff4",
   "metadata": {},
   "source": [
    "### Stemming - `RegexpStemmer`\n",
    "Another useful stemming algorithm is the Regular\r",
    "Expression Stemming algorithm, which takes a single RE and removes\r",
    "prefix or suffix matching that expression. NLTK provides us\r",
    "*RegexpStemmer* class with the help of which we can easily implement this\r",
    "algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df84ff87-c953-4a4c-ae0d-9af73c9e4d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enjoy'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "Regexp_stemmer = RegexpStemmer('ing')\n",
    "\n",
    "Regexp_stemmer.stem('enjoying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea4d7427-f43d-4946-bd16-aec1d71c6cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enjoy'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Regexp_stemmer.stem('ingenjoy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4808ec-6aeb-499e-a687-c7016f2cfb1d",
   "metadata": {},
   "source": [
    "### Stemming - `SnowballStemmer`\n",
    "The Snowball Steaming algorithm supports 15 nonEnglish languages. To work with this algorithm, we first need to create an\r",
    "instance of the language and then call the method *stem()*. NLTKprovides us SnowballStemmer class with the help of which we can easily\r",
    "implement this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df3124c2-bed2-4dc3-9df9-dbd05b387d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "SnowballStemmer.languages #Languge supported by Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da30a02d-9749-4a7e-8fae-ec2103b1fab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bonjour'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Language_French = SnowballStemmer('french')\n",
    "\n",
    "Language_French.stem ('Bonjoura')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75c4db5c-7100-4bb0-9ee0-81f7410e8f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Language_English = SnowballStemmer('english')\n",
    "\n",
    "Language_English.stem ('Eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a99f0e9-202e-4f0c-a015-c719de0c99b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Language_English.stem ('Reading')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9b107-e58b-4c9e-91f4-b2de77273497",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "The lemmatization technique is similar to the stemming technique that we have\r",
    "discussed before but the difference is that lemmatization gives us the root word\r",
    "as output, rather than root stem, which we usually get after stemming. The\r",
    "technical name of the root word is lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a90116a-b676-45c7-a40c-01ac69c566cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reading'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ex_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ex_lemmatizer.lemmatize('reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8ab7fd7-d8a6-497f-8c1d-b605f2a742d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sweet'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_lemmatizer.lemmatize('sweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4530441-6e4d-470c-853e-3d684aea5fe4",
   "metadata": {},
   "source": [
    "### Difference between Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390bca8c-a353-47f4-ba7d-6ae809a7e527",
   "metadata": {},
   "source": [
    "| Feature             | Lemmatization                                        | Stemming                                                |\n",
    "|---------------------|------------------------------------------------------|---------------------------------------------------------|\n",
    "| Valid Word Output   | Lemmatization always provides us a valid word as output. It means that lemma is an actual language word. | Stemming chops off the suffix or prefix, hence it might not provide a valid word as output always. It means that stem might not be an actual word. |\n",
    "| Word Analysis       | Lemmatization looks at the meaning of the word.      | Stemming looks at the form of the word.                 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "737eb211-3bbb-46a8-9e3e-d0d63dd6d98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implementing Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ex_wordstemmer = PorterStemmer()\n",
    "\n",
    "ex_wordstemmer.stem('believe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28db2665-1a6f-46f5-ae20-a52cc6f8c4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' believe '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implementing Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ex_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ex_lemmatizer.lemmatize(' believe ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ed58c-fb66-45f5-8cff-067b235f80c7",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "Chunking, one of the important processes in NLP, is used to extract phrases\n",
    "from the unstructured text. In other words, chunking is used to analyze the\n",
    "structure of a sentence to identify constituents such as noun groups, verb\n",
    "groups, verbs, and so on. Chunking is also called partial parsing and works on\n",
    "top of the POS tagging.\n",
    "\n",
    "Follow the steps to implement noun–phrase chunking in Python:\n",
    "\n",
    "1. Chunk grammar definition: It is the first step for implementing noun–\n",
    "phrase chunking. Here we need to define the grammar for chunking,\n",
    "which would be containing the rules that need to be followed.\n",
    "\n",
    "2. Chunk parser creation: Once you define the chunk grammar, it is time\n",
    "to create a chunk parser, which will parse the grammar and produce the\n",
    "output in tree format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0aac4285-7246-4f01-8b06-d2c4b4d7f931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,256.0,168.0\" width=\"256px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"37.5%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"50%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">This</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"20px\" y2=\"48px\" /><svg width=\"50%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">book</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"18.75%\" y1=\"20px\" y2=\"48px\" /><svg width=\"15.625%\" x=\"37.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">has</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.3125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"46.875%\" x=\"53.125%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"33.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ten</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.6667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"66.6667%\" x=\"33.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">chapters</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.6667%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.5625%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('NP', [('This', 'DT'), ('book', 'NN')]), Tree('VP', [('has', 'VBZ')]), Tree('NP', [('ten', 'JJ'), ('chapters', 'NNS')])])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import Tree\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the input sequence of words and their POS tags\n",
    "S = [(\"This\", \"DT\"), (\"book\", \"NN\"), (\"has\", \"VBZ\"), (\"ten\", \"JJ\"), (\"chapters\", \"NNS\")]\n",
    "\n",
    "# Define a simplified and better grammar for chunking\n",
    "chunker = nltk.RegexpParser(r'''\n",
    "    NP: {<DT>?<JJ>*<NN.*>+}  # Define a noun phrase (optional determiner, optional adjectives, followed by nouns)\n",
    "    VP: {<VB.*>}             # Define a verb phrase (capture verbs)\n",
    "''')\n",
    "\n",
    "# Parse the input sequence\n",
    "output = chunker.parse(S)\n",
    "\n",
    "# Display the parse tree in Jupyter Notebook\n",
    "def draw_tree_in_notebook(tree):\n",
    "    \"\"\"\n",
    "    A function to draw the NLTK Tree in a Jupyter Notebook cell.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))             # Set the figure size for better visibility\n",
    "    tree.draw()                             # Draw the tree (this will open a new window by default)\n",
    "    plt.show()                              # Display the plot inline (works for Jupyter)\n",
    "\n",
    "# Display the parse tree as a string in the notebook using IPython display\n",
    "display(output)\n",
    "\n",
    "# Alternatively, draw the parse tree in notebook\n",
    "#output.pretty_print()  # Print the tree structure in the notebook in text form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "befff905-250d-4c2d-b48b-2a93bc2ac4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expression: '3% 2 Net 10' -> Classification: DF\n",
      "Entities: [('3%', 'PERCENTAGE'), ('2', 'DAYS'), ('Net', 'NET_TERM'), ('10', 'DAYS')]\n",
      "\n",
      "Expression: 'Net 10' -> Classification: NF\n",
      "Entities: [('Net', 'NET_TERM'), ('10', 'DAYS')]\n",
      "\n",
      "Expression: '4% 10 th PR' -> Classification: UNKNOWN\n",
      "Entities: [('4%', 'PERCENTAGE'), ('10', 'DAYS'), ('th', 'ORDER'), ('PR', 'FUTURE')]\n",
      "\n",
      "Expression: '10 Net 30 Days' -> Classification: UNKNOWN\n",
      "Entities: [('10', 'DAYS'), ('Net', 'NET_TERM'), ('30', 'DAYS'), ('Days', 'TIME')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the patterns for each label\n",
    "patterns = {\n",
    "    \"PERCENTAGE\": r\"\\b\\d+%|\\.\\d+%|\\b\\d+\\s*%\",  # Matches numbers followed by '%' (e.g., 3%, .5%)\n",
    "    \"DAYS\": r\"\\b\\d+\\b\",                        # Matches standalone numbers (e.g., 10, 30)\n",
    "    \"ORDER\": r\"\\b(?:st|nd|rd|th)\\b\",           # Matches order indicators (e.g., st, nd, th)\n",
    "    \"NET_TERM\": r\"\\bNet\\b\",                    # Matches the word 'Net'\n",
    "    \"FUTURE\": r\"\\b(?:MF|PR|Prox)\\b\",           # Matches future-related words (e.g., MF, PR, Prox)\n",
    "    \"TIME\": r\"\\b(?:Days|EOM|AF|of|Month)\\b\",   # Matches time-related terms\n",
    "    \"NULL\": r\"\\b(?:AF|KEY|n|T)\\b\"              # Matches miscellaneous terms that fall under 'NULL'\n",
    "}\n",
    "\n",
    "# Function to classify entities in the expression\n",
    "def classify_entities(expression):\n",
    "    tokens = expression.split()  # Tokenize the input expression\n",
    "    classified = []\n",
    "\n",
    "    for token in tokens:\n",
    "        found = False\n",
    "        for label, pattern in patterns.items():\n",
    "            if re.fullmatch(pattern, token, re.IGNORECASE):\n",
    "                classified.append((token, label))\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            classified.append((token, \"UNKNOWN\"))  # In case it doesn't match any pattern\n",
    "\n",
    "    return classified\n",
    "\n",
    "# Function to determine classification of the entire expression\n",
    "def classify_expression(expression):\n",
    "    entities = classify_entities(expression)\n",
    "    labels = [label for _, label in entities]\n",
    "\n",
    "    # Define classification based on label sequence\n",
    "    if labels == [\"PERCENTAGE\", \"DAYS\", \"NET_TERM\", \"DAYS\"]:\n",
    "        classification = \"DF\"\n",
    "    elif labels == [\"NET_TERM\", \"DAYS\"]:\n",
    "        classification = \"NF\"\n",
    "    else:\n",
    "        classification = \"UNKNOWN\"\n",
    "\n",
    "    return classification, entities\n",
    "\n",
    "# Test expressions\n",
    "expressions = [\n",
    "    \"3% 2 Net 10\",    # Expected: DF\n",
    "    \"Net 10\",         # Expected: NF\n",
    "    \"4% 10 th PR\",    # Complex case to test\n",
    "    \"10 Net 30 Days\"  # Complex case to test\n",
    "]\n",
    "\n",
    "# Evaluate test expressions\n",
    "for expr in expressions:\n",
    "    classification, entities = classify_expression(expr)\n",
    "    print(f\"Expression: '{expr}' -> Classification: {classification}\")\n",
    "    print(f\"Entities: {entities}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f90e0-7064-496b-b5ec-f95c0f0bb0c6",
   "metadata": {},
   "source": [
    "### Bag-of-Words(BoW) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d74cd5-b95b-46a2-9937-72138961d47f",
   "metadata": {},
   "source": [
    "Bag-of-Words (BoW) is an NLP technique of text modeling, which is used to\n",
    "extract the features from the text. The BoW is a representation of the text\n",
    "describing the existence of words inside a document. It is called a bag of words\n",
    "because it is only concerned with the occurrence of the words in the document\n",
    "and any kind of information about the structure of words is discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d9f5027-b062-4dde-af1a-466482e4be58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bag': 0, 'of': 7, 'words': 15, 'model': 5, 'is': 4, 'very': 14, 'useful': 13, 'nlp': 6, 'technique': 8, 'used': 12, 'to': 11, 'extract': 1, 'the': 10, 'features': 2, 'from': 3, 'text': 9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "Sentences=['Bag of Words model is very useful NLP technique.', 'Bag of Words model is used to extract the features from text.']\n",
    "\n",
    "vector_count = CountVectorizer()\n",
    "\n",
    "text_feature = vector_count.fit_transform(Sentences).todense()\n",
    "\n",
    "print(vector_count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b541adf-a899-413f-a04d-eb611316d044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1]\n",
      " [1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(text_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b4ab17-ecdf-4f1c-8224-dd75e7d10c01",
   "metadata": {},
   "source": [
    "## Example 1: Predicting the Category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc2197-010c-43b0-a09b-94694f74db76",
   "metadata": {},
   "source": [
    "Every word is important but categorizing the document is equally important.\n",
    "Categorizing a document means in which category of text a word falls. For\n",
    "example, we want to predict the given sentence falls in which category like email, sports, business, and so on. For our following example, we will be using\n",
    "20 newsgroup datasets from the Scikit-learn Python library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a09cd12-b930-4a37-b994-72268b7bab1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training data: (2755, 39297)\n",
      "\n",
      "The Input Data is: Columbia is the name of a space shuttle \n",
      " Category: Space\n",
      "\n",
      "The Input Data is: Hindu, isai, Sikh, Muslim all are religions \n",
      " Category: Religion\n",
      "\n",
      "The Input Data is: We shoul drive safely \n",
      " Category: Autos\n",
      "\n",
      "The Input Data is: Puck is a round disk made of hard rubber \n",
      " Category: Hockey\n",
      "\n",
      "The Input Data is: Television, Microwave, Mixer Grinder, Refrigrator, all uses electricity \n",
      " Category: Electronics\n"
     ]
    }
   ],
   "source": [
    "#Import the required packages\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#Defining five different category maps\n",
    "c_map = {'talk.religion.misc': 'Religion', 'rec.autos': 'Autos','rec.sport.hockey':'Hockey','sci.electronics':'Electronics',        'sci.space': 'Space'} \n",
    "\n",
    "#Creating the training set\n",
    "t_data = fetch_20newsgroups(subset='train', \n",
    "        categories=c_map.keys(), shuffle=True, random_state=5)\n",
    "\n",
    "#Building a count vectorizer and extracting the term counts\n",
    "v_count = CountVectorizer()\n",
    "train_tc = v_count.fit_transform(t_data.data)\n",
    "print(\"\\nDimensions of training data:\", train_tc.shape)\n",
    "\n",
    "#Creating tf-idf transformer\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_tc)\n",
    "\n",
    "#Defining the test data\n",
    "input_data = [\n",
    "    'Columbia is the name of a space shuttle',\n",
    "    'Hindu, isai, Sikh, Muslim all are religions',\n",
    "    'We shoul drive safely',\n",
    "    'Puck is a round disk made of hard rubber',\n",
    "    'Television, Microwave, Mixer Grinder, Refrigrator, all uses electricity']\n",
    "\n",
    "#Multimonial Naïve Bayes classifier training\n",
    "classifier = MultinomialNB().fit(train_tfidf, t_data.target)\n",
    "\n",
    "#Transforming input data by using count vectorizer\n",
    "input_tc = v_count.transform(input_data)\n",
    "\n",
    "#Transforming vectorized data by using tf-idf transformer\n",
    "input_tfidf = tfidf.transform(input_tc)\n",
    "\n",
    "#Predicting output categories\n",
    "predictions = classifier.predict(input_tfidf)\n",
    "for sent, category in zip(input_data, predictions):\n",
    "    print('\\nThe Input Data is:', sent, '\\n Category:', \\\n",
    "            c_map[t_data.target_names[category]])   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b3a09e-9353-47a1-aeed-dbf2d2ecb198",
   "metadata": {},
   "source": [
    "#### Summary of the Code Workflow\n",
    "\n",
    "* `Category Mapping`: Maps the newsgroup categories to readable names.\n",
    "* `Training Data`: Loads a subset of the 20 Newsgroups dataset.\n",
    "* `Feature Extraction`: Converts text data into term counts and TF-IDF scores.\n",
    "* `Classifier Training`: Trains a Multinomial Naive Bayes model on the TF-IDF scores.\n",
    "* `Testing`: Converts input samples into TF-IDF representation and predicts the categories using the trained classifier.\n",
    "* `Print Results`: Displays each input sample and its predicted category.\n",
    "\n",
    "The code demonstrates a classic text classification pipeline, starting from preprocessing and ending with classification and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85272e74-f29b-4097-8327-ebc08e6e7943",
   "metadata": {},
   "source": [
    "## Example 2: Gender Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a071f-7441-40f3-909c-39cb6d053a01",
   "metadata": {},
   "source": [
    "This example is to train a classifier that will predict the gender by providing\n",
    "names of males and females. First, the classifier will decide what features of\n",
    "the input are valid. Second, the classifier will decide how it can encode those\n",
    "features. We need to create a feature extractor function that will build a\n",
    "dictionary containing appropriate data about a given name (male or female)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ae2c4dd-03fb-412d-a1c7-c074f5c4757d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aarav is a male.\n",
      "Shilpi is a male.\n",
      "\n",
      "Accuracy: 1.0\n",
      "\n",
      "Most Informative Features\n",
      "             last_letter = 'A'            female : male   =     50.3 : 1.0\n",
      "             last_letter = 'O'              male : female =     13.6 : 1.0\n",
      "             last_letter = 'D'              male : female =     13.5 : 1.0\n",
      "             last_letter = 'G'              male : female =      8.3 : 1.0\n",
      "             last_letter = 'I'            female : male   =      7.2 : 1.0\n",
      "             last_letter = 'T'              male : female =      4.2 : 1.0\n",
      "             last_letter = 'R'              male : female =      3.9 : 1.0\n",
      "             last_letter = 'M'              male : female =      3.8 : 1.0\n",
      "             last_letter = 'S'              male : female =      3.2 : 1.0\n",
      "             last_letter = 'L'              male : female =      2.7 : 1.0\n",
      "             last_letter = 'N'              male : female =      2.4 : 1.0\n",
      "             last_letter = 'E'            female : male   =      2.3 : 1.0\n",
      "             last_letter = 'Z'            female : male   =      1.8 : 1.0\n",
      "             last_letter = 'H'              male : female =      1.7 : 1.0\n",
      "             last_letter = 'U'            female : male   =      1.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Import the required packages\n",
    "import pandas as pd\n",
    "import random\n",
    "from nltk.corpus import names\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "\n",
    "\n",
    "# URL to the text file\n",
    "url1 = \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Usernames/Names/malenames-usa-top1000.txt\"\n",
    "url2 = \"https://raw.githubusercontent.com/danielmiessler/SecLists/refs/heads/master/Usernames/Names/femalenames-usa-top1000.txt\"\n",
    "\n",
    "# Read the text file using read_table, specifying no header and a line terminator\n",
    "males = pd.read_table(url1, header=None, names=[\"Name\"])\n",
    "females = pd.read_table(url2, header=None, names=[\"Name\"])\n",
    "\n",
    "# Create a list of tuples with each name labeled as 'male'\n",
    "names_M = [(name, 'male') for name in males['Name']]\n",
    "names_F = [(name, 'female') for name in females['Name']]\n",
    "\n",
    "names_labels = names_M + names_F\n",
    "random.shuffle(names_labels)\n",
    "\n",
    "#Defining the function to calculate features\n",
    "def features(word):\n",
    "      return {'last_letter': word[-1]}\n",
    "featuresets = [(features(n), gender) for (n, gender) in names_labels]\n",
    "\n",
    "# Splitting  the dataset into training set and testing set.\n",
    "train_set, test_set = featuresets[5:], featuresets[:5]\n",
    "\n",
    "# Training the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "male_gender = classifier.classify(features('Aarav'))\n",
    "female_gender = classifier.classify(features('Shilpi'))\n",
    "\n",
    "# Tests\n",
    "print(\"Aarav is a {}.\".format(male_gender))\n",
    "print(\"Shilpi is a {}.\".format(female_gender))\n",
    "print()\n",
    "\n",
    "#Getting the accuracy\n",
    "print(\"Accuracy:\",accuracy(classifier, test_set))\n",
    "print()\n",
    "\n",
    "#Printing first 15 feature sets\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b4783d-3763-456d-9cb3-c7d2f7f88c30",
   "metadata": {},
   "source": [
    "#### Summary of the Code Workflow:\n",
    "\n",
    "* `Load Name Data`: Reads in lists of male and female names from two text files.\n",
    "* `Label Names`: Labels each name with either 'male' or 'female'.\n",
    "* `Feature Extraction`: Extracts the last letter of each name as the feature for classification.\n",
    "* `Training and Testing`: Splits the dataset into training and testing sets.\n",
    "* `Train Classifier`: Trains a Naive Bayes classifier using the training set.\n",
    "* `Predict Gende`r: Uses the trained classifier to predict the gender of new names.\n",
    "* `Evaluate Performance`: Calculates the accuracy of the classifier and shows the most informative features.\n",
    "\n",
    "This code shows how a simple feature (the last letter of a name) can be used to build a gender classifier using a Naive Bayes approach. The feature extraction method is basic but works surprisingly well for this kind of text classification task, as shown by the informative feature output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
