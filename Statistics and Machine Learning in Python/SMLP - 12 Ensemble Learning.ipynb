{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e8aa49-4745-40ed-80fb-c52cb7510fa9",
   "metadata": {},
   "source": [
    "# Chapter 5.7 - Ensemble Learning: Bagging, Boosting and Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2711ad-43d7-450f-b623-b44e12bec9bd",
   "metadata": {},
   "source": [
    "These methods are **Ensemble Learning** techniques. These models are machine learning\n",
    "paradigms where multiple models (often called “weak learners”) are trained to **solve the same\n",
    "problem** and **combined** to get **better** results. The main hypothesis is that when **weak models**\n",
    "are **correctly combined** we can obtain **more accurate and/or robust models**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be2eff-ea06-4b32-9cf4-632a1a2c04d4",
   "metadata": {},
   "source": [
    "Usually, ensemble models are used in order to :\n",
    "\n",
    "    • decrease the variance for bagging (Bootstrap Aggregating) technique\n",
    "    • reduce bias for the boosting technique\n",
    "    • improving the predictive force for stacking technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ebff2c-7587-4745-95f9-619d08532096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a54416-d40f-4920-8e67-c4d9e2578dfc",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "(Standing for “**b**ootstrap **aggr**egat**ing**”). Aims at producing an ensemble model that is more\r",
    "robust than the individual models composing it.\n",
    "\n",
    "The idea of bagging is then simple: we want to fit several independent models and “average”\r",
    "their predictions in order to obtain a model with a lower variance. However, we can’t, in\r",
    "practice, fit fully independent models because it would require too much data. So, we rely on\r",
    "the good “approximate properties” of bootstrap samples (representativity and independence)\r",
    "to fit models that are almost independent.\n",
    "\n",
    "Bagging consists in fitting several base models on different bootstrap samples and build an\r",
    "ensemble model that “average” the results of these weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db5cd72-d35a-446c-a46e-73dcc504539e",
   "metadata": {},
   "source": [
    "### Bagged Decision Trees for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f17a742-bcb8-41eb-9df6-176d5fa8ad85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  test  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50      1\n",
       "1     1    85    66    29     0  26.6  0.351   31      0\n",
       "2     8   183    64     0     0  23.3  0.672   32      1\n",
       "3     1    89    66    23    94  28.1  0.167   21      0\n",
       "4     0   137    40    35   168  43.1  2.288   33      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = pd.read_csv(url,names=names)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f3c4d20-d2ea-4e00-9e30-00812526d093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "array = df.values  # Convert the pandas DataFrame 'df' into a NumPy array.\n",
    "x = array[:, 0:8]  # Select all rows and columns 0 to 7 (features) from the array (first 8 columns).\n",
    "y = array[:, 8]    # Select all rows and column 8 (target/label) from the array (9th column, used for classification).\n",
    "\n",
    "max_features = 3    # Set the maximum number of features that each base estimator (decision tree) can use for splitting.\n",
    "\n",
    "# Define the cross-validation strategy using KFold \n",
    "kfold = model_selection.KFold(n_splits=10,              # 10 splits - each fold will be split into training and testing sets.\n",
    "                              shuffle=True,             # Shuffle the data before splitting into folds.\n",
    "                              random_state=2020)        # Use a fixed random state for reproducibility.\n",
    "\n",
    "# Initialize a decision tree classifier as the base estimator for bagging\n",
    "rf = DecisionTreeClassifier(max_features=max_features)  # limit of 3 features for splitting.\n",
    "\n",
    "num_trees = 100   # Define the number of trees (or estimators) in the bagging ensemble model.\n",
    "\n",
    "# Initialize a bagging classifier with 100 base estimators (decision trees) and a fixed random state.\n",
    "model = BaggingClassifier(estimator=rf,          # Base estimator: decision tree classifier\n",
    "                          n_estimators=num_trees,  # Define the number of trees (100)\n",
    "                          random_state=2020)       # The bagging model will use decision trees as its base estimator.\n",
    "\n",
    "# Perform 10-fold cross-validation on the model using the feature set 'x' and target labels 'y'.\n",
    "results = model_selection.cross_val_score(model, x, y, cv=kfold)\n",
    "\n",
    "# Print the mean accuracy and standard deviation of the model across the 10 folds.\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (results.mean(), results.std()))  # Results are reported as mean accuracy +/- standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a58b525-6c49-48d1-b402-adc78e9bcd64",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d826c40b-4bdf-41e7-aace-bc2b92a32200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"  # URL of the Pima Indians Diabetes dataset\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']  # Column names for the dataset\n",
    "df = pd.read_csv(url, names=names)  # Load the dataset from the URL into a pandas DataFrame\n",
    "\n",
    "# Convert the DataFrame into a NumPy array\n",
    "array = df.values  # Extract the values of the DataFrame as a NumPy array\n",
    "x = array[:, 0:8]  # Select all rows and the first 8 columns (features) for input (X)\n",
    "y = array[:, 8]    # Select all rows and the 9th column (target labels: class) for output (Y)\n",
    "\n",
    "# Define the cross-validation strategy using KFold\n",
    "kfold = model_selection.KFold(n_splits=10,        # 10 splits - each fold will be split into training and testing sets.\n",
    "                              shuffle=True,       # Shuffle enabled\n",
    "                              random_state=2020)  # Use a fixed random state for reproducibility. \n",
    "\n",
    "# Specify the number of trees (estimators) for the RandomForest model\n",
    "num_trees = 100  # Use 100 decision trees in the RandomForest ensemble\n",
    "\n",
    "# Specify the maximum number of features used to split each node in the RandomForest model\n",
    "max_features = 3  # Use 3 features for splitting each node in the trees\n",
    "\n",
    "# Initialize the RandomForestClassifier model\n",
    "model = RandomForestClassifier(n_estimators=num_trees,     # 100 trees\n",
    "                               max_features=max_features)  # limit to 3 features per split\n",
    "\n",
    "# Perform cross-validation on the RandomForest model\n",
    "results = model_selection.cross_val_score(model, x, y, cv=kfold)        # Perform cross-validation and store the results (accuracy for each fold)\n",
    "\n",
    "# Print the mean accuracy and standard deviation from the 10-fold cross-validation\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (results.mean(), results.std()))  # Print the average accuracy and its standard deviation across the folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb097c-5ece-4303-93ae-53d542afcb6a",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "In **sequential methods** the different combined weak models are **no longer** fitted **independently** from each others. The idea is to fit models **iteratively** such that the training of model at a given step depends on the models fitted at the previous steps. “Boosting” is the most famous\r",
    "of these approaches and it produces an ensemble model that is in general **less biased** than the weak learners that compose it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d983cb-c1eb-4414-8a5d-7f311ee74501",
   "metadata": {},
   "source": [
    "Boosting methods work in the same spirit as bagging methods: we build a family of models\n",
    "that are aggregated to obtain a strong learner that performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de7c6d9-37ca-4906-b836-b8efca234aaf",
   "metadata": {},
   "source": [
    "However, unlike bagging that mainly aims at reducing variance, boosting is a technique\n",
    "that consists in fitting sequentially multiple weak learners in a very adaptative way: each\n",
    "model in the sequence is fitted giving more importance to observations in the dataset that\n",
    "were badly handled by the previous models in the sequence. Intuitively, each new model\n",
    "focus its efforts on the most difficult observations to fit up to now, so that we obtain, at the\n",
    "end of the process, a strong learner with **lower bias** (even if we can notice that boosting can\n",
    "also have the effect of reducing variance).\n",
    "\n",
    "Boosting consists in, iteratively, fitting a weak learner, aggregate it to the ensemble model and\r",
    "“update” the training dataset to better take into account the strengths and weakness of the\r",
    "current ensemble model when fitting the next base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f264088-e1e7-4f0b-80d7-d849eb60377a",
   "metadata": {},
   "source": [
    "### Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae85d566-b7e8-434b-ada1-57de450b997f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Boosting : F1 Score 0.93, Accuracy 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Load the breast cancer dataset from sklearn\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "# Convert the data into a DataFrame for easier manipulation, using feature names as column headers\n",
    "x = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "\n",
    "# Convert the target into a categorical type, using the target names as labels\n",
    "y = pd.Categorical.from_codes(breast_cancer.target, breast_cancer.target_names)\n",
    "\n",
    "# Label encode the target to convert it into integer labels (0 and 1)\n",
    "encoder = LabelEncoder()\n",
    "binary_encoded_y = pd.Series(encoder.fit_transform(y))  # Fit the encoder and transform the labels to integers\n",
    "\n",
    "# Split the dataset into training and testing sets, using 75% for training and 25% for testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, binary_encoded_y, random_state=1)\n",
    "\n",
    "# Initialize an AdaBoost classifier\n",
    "clf_boosting = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),  # Base estimator is a decision tree with a maximum depth of 1\n",
    "    n_estimators=200,                     # Use 200 decision trees (weak learners) in the AdaBoost ensemble\n",
    "    algorithm='SAMME'                     # Use SAMME to avoid the warning\n",
    ")\n",
    "\n",
    "# Fit the AdaBoost classifier to the training data\n",
    "clf_boosting.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = clf_boosting.predict(test_x)\n",
    "\n",
    "# Print the F1 score and accuracy of the model\n",
    "print(\"For Boosting : F1 Score {}, Accuracy {}\".format(\n",
    "    round(f1_score(test_y, predictions), 2),       # Compute the F1 score (rounded to 2 decimal places)\n",
    "    round(accuracy_score(test_y, predictions), 2)  # Compute the accuracy score (rounded to 2 decimal places)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1cf7f-e12d-461f-9c3c-5ae91dced135",
   "metadata": {},
   "source": [
    "### Random Forest as a Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd9e4a5a-c2ec-4572-8198-c0494e4809b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Bagging : F1 Score 0.85, Accuracy 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Load the breast cancer dataset from sklearn\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "# Convert the breast cancer data into a pandas DataFrame with feature names as columns\n",
    "x = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "\n",
    "# Convert the target values into categorical labels using the target names\n",
    "y = pd.Categorical.from_codes(breast_cancer.target, breast_cancer.target_names)\n",
    "\n",
    "# Transforming string target (i.e., 'malignant', 'benign') to an integer (0 or 1)\n",
    "encoder = LabelEncoder()                                # Initialize the LabelEncoder\n",
    "binary_encoded_y = pd.Series(encoder.fit_transform(y))  # Encode target labels to integers and store in a pandas Series\n",
    "\n",
    "# Split the dataset into training and testing sets (75% train, 25% test)\n",
    "train_x, test_x, train_y, test_y = train_test_split(x,                 # Features\n",
    "                                                    binary_encoded_y,  # Target labels\n",
    "                                                    random_state=1)    # Set random_state for reproducibility\n",
    "\n",
    "# Initialize a RandomForestClassifier with 200 decision trees and max depth of 1 (similar to bagging with weak learners)\n",
    "clf_bagging = RandomForestClassifier(n_estimators=200,  # Number of decision trees (estimators)\n",
    "                                     max_depth=1)       # Set max depth of the decision trees to 1 (weak learners)\n",
    "\n",
    "# Train (fit) the RandomForestClassifier on the training data\n",
    "clf_bagging.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = clf_bagging.predict(test_x)\n",
    "\n",
    "# Print the F1 score and accuracy score of the model\n",
    "print(\"For Bagging : F1 Score {}, Accuracy {}\".format(\n",
    "    round(f1_score(test_y, predictions), 2),       # Calculate F1 score and round to 2 decimal places\n",
    "    round(accuracy_score(test_y, predictions), 2)  # Calculate accuracy score and round to 2 decimal places\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c99f39-3c56-45d9-a2ee-8bdcce3f8b1e",
   "metadata": {},
   "source": [
    "| Metric | Bagging | Boosting |\n",
    "|:---------|:--------:|---------:|\n",
    "|  Accuracy   |  0.91   |  0.95   |\n",
    "|  F1-Score   |  0.88   |  0.93   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c3129b-50f5-49b7-bfa4-0ad00e285b8f",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef8688-c8cd-4589-9ee7-4885307903ef",
   "metadata": {},
   "source": [
    "**Stacking** mainly differ from **Bagging** and **Boosting** on two points : - First stacking often considers heterogeneous weak learners (different learning algorithms are combined) whereas bagging and boosting consider mainly homogeneous weak learners. - Second, stacking learns\n",
    "to combine the base models using a meta-model whereas bagging and boosting combine weak learners following deterministic algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "755976db-0a02-47b1-b2d5-3011b7a45ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Bagging : F1 Score 0.88, Accuracy 0.9\n",
      "For Boosting : F1 Score 0.93, Accuracy 0.94\n",
      "For Stacking : F1 Score 0.98, Accuracy 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "# Load the breast cancer dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "# Create a DataFrame for the features of the breast cancer dataset\n",
    "x = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names) \n",
    "\n",
    "# Create a Categorical object for the target values\n",
    "y = pd.Categorical.from_codes(breast_cancer.target, breast_cancer.target_names)  # Convert the target labels to categorical format\n",
    "\n",
    "# Encode the categorical target labels into binary (0 or 1)\n",
    "encoder = LabelEncoder()                                # Initialize the label encoder\n",
    "binary_encoded_y = pd.Series(encoder.fit_transform(y))  # Apply the encoder to transform the target labels into binary format\n",
    "\n",
    "# Split the dataset into training and testing sets (75% train, 25% test)\n",
    "train_x, test_x, train_y, test_y = train_test_split(x,  # Input features\n",
    "                                                    binary_encoded_y,   # Target labels\n",
    "                                                    random_state=2020)  # Use a fixed random state for reproducibility\n",
    "\n",
    "# Initialize the AdaBoostClassifier with a DecisionTreeClassifier as the base estimator (with a maximum depth of 1)\n",
    "boosting_clf_ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),  # Weak learner for boosting (decision stump)\n",
    "                                            n_estimators=3,                       # Use 3 weak learners (decision stumps) in boosting\n",
    "                                            algorithm='SAMME')                    # Use SAMME algorithm to avoid the FutureWarning\n",
    "\n",
    "# Initialize the RandomForestClassifier for bagging (using decision trees with a max depth of 1)\n",
    "bagging_clf_rf = RandomForestClassifier(n_estimators=200,   # Use 200 decision trees in the RandomForest model\n",
    "                                        max_depth=1,        # Use weak learners (max depth = 1) for each tree\n",
    "                                        random_state=2020)  # Use a fixed random state for reproducibility\n",
    "\n",
    "# Another RandomForestClassifier to use for stacking (same settings as bagging_clf_rf)\n",
    "clf_rf = RandomForestClassifier(n_estimators=200,   # 200 decision trees for RandomForest\n",
    "                                max_depth=1,        # Weak learners (max depth = 1)\n",
    "                                random_state=2020)  # Use fixed random state for reproducibility\n",
    "\n",
    "# Initialize the AdaBoostClassifier for stacking\n",
    "clf_ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1,         # Weak learner for boosting (decision stump)\n",
    "                                                          random_state=2020),  # Fixed random state\n",
    "                                   n_estimators=3,     # 3 weak learners (decision stumps)\n",
    "                                   algorithm='SAMME')  # Use SAMME algorithm to avoid the warning\n",
    "\n",
    "# Initialize LogisticRegression for stacking as the final classifier (meta-learner)\n",
    "clf_logistic_reg = LogisticRegression(solver='liblinear',  # Use liblinear solver for binary classification\n",
    "                                      random_state=2020)   # Use fixed random state for reproducibility\n",
    "\n",
    "# Custom exception to handle cases where the number of classifiers in stacking is less than 2\n",
    "class NumberOfClassifierException(Exception):\n",
    "    pass  # Define a custom exception to raise when the number of classifiers is insufficient\n",
    "\n",
    "# Create a stacking class\n",
    "class Stacking():\n",
    "    '''\n",
    "    This is a test class for stacking! Please feel free to modify it to fit your needs.\n",
    "    We assume that at least the first N-1 classifiers have a predict_proba function.\n",
    "    '''\n",
    "    def __init__(self,classifiers):\n",
    "        # Ensure there are at least 2 classifiers\n",
    "        if(len(classifiers) < 2):                                                                          # Raise an error if there are not \n",
    "            raise NumberOfClassifierException(\"You must fit your classifier with at least 2 classifiers\")  # enough classifiers\n",
    "        else:\n",
    "            self._classifiers = classifiers  # Store the list of classifiers\n",
    "        \n",
    "    def fit(self, data_x, data_y):\n",
    "        # Initialize stacked data with original features\n",
    "        stacked_data_x = data_x.copy()       # Create a copy of the input data (features)\n",
    "        \n",
    "        # Train N-1 classifiers and stack their predicted probabilities\n",
    "        for classifier in self._classifiers[:-1]:\n",
    "            classifier.fit(data_x, data_y)  # Fit each classifier to the data\n",
    "            stacked_data_x = np.column_stack((stacked_data_x, classifier.predict_proba(data_x))) # Stack the predicted probabilities of each classifier\n",
    "        \n",
    "        # Fit the final (meta) classifier on the stacked data\n",
    "        last_classifier = self._classifiers[-1]      # Select the last classifier (meta-learner)\n",
    "        last_classifier.fit(stacked_data_x, data_y)  # Fit the meta-learner on the stacked features and probabilities\n",
    "        \n",
    "    def predict(self, data_x):\n",
    "        # Initialize stacked data with original features\n",
    "        stacked_data_x = data_x.copy()  # Create a copy of the input data (features)\n",
    "        \n",
    "        # Stack predictions from N-1 classifiers\n",
    "        for classifier in self._classifiers[:-1]:\n",
    "            prob_predictions = classifier.predict_proba(data_x)                   # Get predicted probabilities\n",
    "            stacked_data_x = np.column_stack((stacked_data_x, prob_predictions))  # Stack the predicted probabilities\n",
    "        \n",
    "        # Use the final (meta) classifier to make the final predictions\n",
    "        last_classifier = self._classifiers[-1]         # Select the last classifier (meta-learner)\n",
    "        return last_classifier.predict(stacked_data_x)  # Return the predictions made by the meta-learner\n",
    "\n",
    "# Train the bagging classifier on the training data\n",
    "bagging_clf_rf.fit(train_x, train_y)          # Fit the RandomForest bagging model on the training data\n",
    "\n",
    "# Train the boosting classifier on the training data\n",
    "boosting_clf_ada_boost.fit(train_x, train_y)  # Fit the AdaBoost model on the training data\n",
    "\n",
    "# List of classifiers to use for stacking (RandomForest, AdaBoost, and LogisticRegression)\n",
    "classifiers_list = [clf_rf, clf_ada_boost, clf_logistic_reg]  # Define the classifiers used in stacking\n",
    "\n",
    "# Create an instance of the Stacking class\n",
    "clf_stacking = Stacking(classifiers_list)  # Create a Stacking model with the classifiers\n",
    "clf_stacking.fit(train_x, train_y)         # Train the stacking model on the training data\n",
    "\n",
    "# Make predictions on the test data using the bagging model\n",
    "predictions_bagging = bagging_clf_rf.predict(test_x)           # Predict the test labels using the bagging model\n",
    "\n",
    "# Make predictions on the test data using the boosting model\n",
    "predictions_boosting = boosting_clf_ada_boost.predict(test_x)  # Predict the test labels using the boosting model\n",
    "\n",
    "# Make predictions on the test data using the stacking model\n",
    "predictions_stacking = clf_stacking.predict(test_x)            # Predict the test labels using the stacking model\n",
    "\n",
    "# Calculate the F1 score and accuracy\n",
    "bagging_f1 = f1_score(test_y, predictions_bagging)         # Calculate the F1 score for the bagging model\n",
    "bagging_ac = accuracy_score(test_y, predictions_bagging)   # Calculate the accuracy for the bagging model\n",
    "boosting_f1 = f1_score(test_y, predictions_boosting)       # Calculate the F1 score for the boosting model\n",
    "boosting_ac = accuracy_score(test_y, predictions_boosting) # Calculate the accuracy for the boosting model\n",
    "stacking_f1 = f1_score(test_y, predictions_stacking)       # Calculate the F1 score for the stacking model\n",
    "stacking_ac = accuracy_score(test_y, predictions_stacking) # Calculate the accuracy for the stacking model\n",
    "\n",
    "# Print the F1 score and accuracy for the bagging, boosting, and stacking models\n",
    "print(\"For Bagging : F1 Score {}, Accuracy {}\".format(round(bagging_f1, 2), round(bagging_ac, 2)))     # Print F1 and accuracy for bagging\n",
    "print(\"For Boosting : F1 Score {}, Accuracy {}\".format(round(boosting_f1, 2), round(boosting_ac, 2)))  # Print F1 and accuracy for boosting\n",
    "print(\"For Stacking : F1 Score {}, Accuracy {}\".format(round(stacking_f1, 2), round(stacking_ac, 2)))  # Print F1 and accuracy for stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab141f41-0a65-420e-ae8c-193033d0c537",
   "metadata": {},
   "source": [
    "| Metric | Bagging | Boosting | Stacking |\n",
    "|:---------|:--------:|:--------:|---------:|\n",
    "|  Accuracy   |  0.90   |  0.94   |  0.98   |\n",
    "|  F1-Score   |  0.88   |  0.93   |  0.98   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
